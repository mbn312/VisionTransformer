{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO80axM1xoR1honK3wNxZ+U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"39_bAtOyPoIA","executionInfo":{"status":"ok","timestamp":1710424926884,"user_tz":360,"elapsed":15830,"user":{"displayName":"Matt Nguyen","userId":"06637147104061724739"}},"outputId":"e26e2f93-609a-423e-a5c5-d8cf521700dc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fdd0c1a1710>"]},"metadata":{},"execution_count":1}],"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as T\n","from torch.optim import Adam\n","from torch.utils.data import DataLoader\n","from torchvision.datasets.mnist import MNIST\n","import numpy as np"]},{"cell_type":"markdown","source":["# Patch Embeddings"],"metadata":{"id":"r5od6MAVPvMp"}},{"cell_type":"code","source":["class PatchEmbedding(nn.Module):\n","  def __init__(self, d_model, img_size, patch_size, n_channels):\n","    super().__init__()\n","\n","    self.d_model = d_model # Dimensionality of Model\n","    self.img_size = img_size # Image Size\n","    self.patch_size = patch_size # Patch Size\n","    self.n_channels = n_channels # Number of Channels\n","\n","    self.linear_project = nn.Conv2d(self.n_channels, self.d_model, kernel_size=self.patch_size, stride=self.patch_size)\n","\n","  # B: Batch Size\n","  # C: Image Channels\n","  # H: Image Height\n","  # W: Image Width\n","  # P_col: Patch Column\n","  # P_row: Patch Row\n","  def forward(self, x):\n","    x = self.linear_project(x) # (B, C, H, W) -> (B, d_model, P_col, P_row)\n","\n","    x = x.flatten(2) # (B, d_model, P_col, P_row) -> (B, d_model, P)\n","\n","    x = x.transpose(-2, -1) # (B, d_model, P) -> (B, P, d_model)\n","\n","    return x"],"metadata":{"id":"MOSovXJoPxws"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Class Token and Positional Encoding"],"metadata":{"id":"t6RwOLUuP0RE"}},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","  def __init__(self, d_model, max_seq_length):\n","    super().__init__()\n","\n","    self.cls_token = nn.Parameter(torch.randn(1, 1, d_model)) # Classification Token\n","\n","    # Creating positional encoding\n","    pe = torch.zeros(max_seq_length, d_model)\n","\n","    for pos in range(max_seq_length):\n","      for i in range(d_model):\n","        if i % 2 == 0:\n","          pe[pos][i] = np.sin(pos/(10000 ** (i/d_model)))\n","        else:\n","          pe[pos][i] = np.cos(pos/(10000 ** ((i-1)/d_model)))\n","\n","    self.register_buffer('pe', pe.unsqueeze(0))\n","\n","  def forward(self, x):\n","    # Expand to have class token for every image in batch\n","    tokens_batch = self.cls_token.expand(x.size()[0], -1, -1)\n","\n","    # Adding class tokens to the beginning of each embedding\n","    x = torch.cat((tokens_batch,x), dim=1)\n","\n","    # Add positional encoding to embeddings\n","    x = x + self.pe\n","\n","    return x"],"metadata":{"id":"KFrdTG6tP13V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Multi-Head Attention"],"metadata":{"id":"FN9_RMizP6TW"}},{"cell_type":"code","source":["class AttentionHead(nn.Module):\n","  def __init__(self, d_model, head_size):\n","    super().__init__()\n","    self.head_size = head_size\n","\n","    self.query = nn.Linear(d_model, head_size)\n","    self.key = nn.Linear(d_model, head_size)\n","    self.value = nn.Linear(d_model, head_size)\n","\n","  def forward(self, x):\n","    # Obtaining Queries, Keys, and Values\n","    Q = self.query(x)\n","    K = self.key(x)\n","    V = self.value(x)\n","\n","    # Dot Product of Queries and Keys\n","    attention = Q @ K.transpose(-2,-1)\n","\n","    # Scaling\n","    attention = attention / (self.head_size ** 0.5)\n","\n","    attention = torch.softmax(attention, dim=-1)\n","\n","    attention = attention @ V\n","\n","    return attention"],"metadata":{"id":"3jQMJjrVP7El"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","  def __init__(self, d_model, n_heads):\n","    super().__init__()\n","    self.head_size = d_model // n_heads\n","\n","    self.W_o = nn.Linear(d_model, d_model)\n","\n","    self.heads = nn.ModuleList([AttentionHead(d_model, self.head_size) for _ in range(n_heads)])\n","\n","  def forward(self, x):\n","    # Combine attention heads\n","    out = torch.cat([head(x) for head in self.heads], dim=-1)\n","\n","    out = self.W_o(out)\n","\n","    return out"],"metadata":{"id":"myuSl3Y2P7ZT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Transformer Encoder"],"metadata":{"id":"ClcjVdqUQMoB"}},{"cell_type":"code","source":["class TransformerEncoder(nn.Module):\n","  def __init__(self, d_model, n_heads, r_mlp=4):\n","    super().__init__()\n","    self.d_model = d_model\n","    self.n_heads = n_heads\n","\n","    # Sub-Layer 1 Normalization\n","    self.ln1 = nn.LayerNorm(d_model)\n","\n","    # Multi-Head Attention\n","    self.mha = MultiHeadAttention(d_model, n_heads)\n","\n","    # Sub-Layer 2 Normalization\n","    self.ln2 = nn.LayerNorm(d_model)\n","\n","    # Multilayer Perception\n","    self.mlp = nn.Sequential(\n","        nn.Linear(d_model, d_model*r_mlp),\n","        nn.GELU(),\n","        nn.Linear(d_model*r_mlp, d_model)\n","    )\n","\n","  def forward(self, x):\n","    # Residual Connection After Sub-Layer 1\n","    out = x + self.mha(self.ln1(x))\n","\n","    # Residual Connection After Sub-Layer 2\n","    out = out + self.mlp(self.ln2(out))\n","\n","    return out"],"metadata":{"id":"nR2pP8B9QOGY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Vision Transformer Model"],"metadata":{"id":"oRlHsYkZQRNz"}},{"cell_type":"code","source":["class VisionTransformer(nn.Module):\n","  def __init__(self, d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers):\n","    super().__init__()\n","\n","    assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \"img_size dimensions must be divisible by patch_size dimensions\"\n","    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n","\n","    self.d_model = d_model # Dimensionality of model\n","    self.n_classes = n_classes # Number of classes\n","    self.img_size = img_size # Image size\n","    self.patch_size = patch_size # Patch size\n","    self.n_channels = n_channels # Number of channels\n","    self.n_heads = n_heads # Number of attention heads\n","\n","    self.n_patches = (self.img_size[0] * self.img_size[1]) // (self.patch_size[0] * self.patch_size[1])\n","    self.max_seq_length = self.n_patches + 1\n","\n","    self.patch_embedding = PatchEmbedding(self.d_model, self.img_size, self.patch_size, self.n_channels)\n","    self.positional_encoding = PositionalEncoding( self.d_model, self.max_seq_length)\n","    self.transformer_encoder = nn.Sequential(*[TransformerEncoder( self.d_model, self.n_heads) for _ in range(n_layers)])\n","\n","    # Classification MLP\n","    self.classifier = nn.Sequential(\n","        nn.Linear(self.d_model, self.n_classes),\n","        nn.Softmax(dim=-1)\n","    )\n","\n","  def forward(self, images):\n","    x = self.patch_embedding(images)\n","\n","    x = self.positional_encoding(x)\n","\n","    x = self.transformer_encoder(x)\n","\n","    x = self.classifier(x[:,0])\n","\n","    return x"],"metadata":{"id":"UDUqSKtwQTQs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training Parameters"],"metadata":{"id":"8ap6w0EmQbMB"}},{"cell_type":"code","source":["d_model = 9\n","n_classes = 10\n","img_size = (32,32)\n","patch_size = (16,16)\n","n_channels = 1\n","n_heads = 3\n","n_layers = 3\n","batch_size = 128\n","epochs = 5\n","alpha = 0.005"],"metadata":{"id":"zGxBtIXiQb1s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load MNIST Dataset"],"metadata":{"id":"yOATVnjpQd2R"}},{"cell_type":"code","source":["transform = T.Compose([\n","  T.Resize(img_size),\n","  T.ToTensor()\n","])\n","\n","train_set = MNIST(\n","  root=\"./../datasets\", train=True, download=True, transform=transform\n",")\n","test_set = MNIST(\n","  root=\"./../datasets\", train=False, download=True, transform=transform\n",")\n","\n","train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n","test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yi2nvpZ4QfqH","executionInfo":{"status":"ok","timestamp":1710425337144,"user_tz":360,"elapsed":1337,"user":{"displayName":"Matt Nguyen","userId":"06637147104061724739"}},"outputId":"85974779-ac62-4573-8324-d2c0070d096c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./../datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 237856843.49it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./../datasets/MNIST/raw/train-images-idx3-ubyte.gz to ./../datasets/MNIST/raw\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./../datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 23996769.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./../datasets/MNIST/raw/train-labels-idx1-ubyte.gz to ./../datasets/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./../datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 77359829.49it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./../datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to ./../datasets/MNIST/raw\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./../datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 4382454.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./../datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./../datasets/MNIST/raw\n","\n"]}]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"w3GesD8vQhcj"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n","\n","transformer = VisionTransformer(d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers).to(device)\n","\n","optimizer = Adam(transformer.parameters(), lr=alpha)\n","criterion = nn.CrossEntropyLoss()\n","\n","for epoch in range(epochs):\n","\n","  training_loss = 0.0\n","  for i, data in enumerate(train_loader, 0):\n","    inputs, labels = data\n","    inputs, labels = inputs.to(device), labels.to(device)\n","\n","    optimizer.zero_grad()\n","\n","    outputs = transformer(inputs)\n","    loss = criterion(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","\n","    training_loss += loss.item()\n","\n","  print(f'Epoch {epoch + 1}/{epochs} loss: {training_loss  / len(train_loader) :.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hKkxpNwyQi70","executionInfo":{"status":"ok","timestamp":1710425455416,"user_tz":360,"elapsed":116683,"user":{"displayName":"Matt Nguyen","userId":"06637147104061724739"}},"outputId":"8f4468d2-132d-4b89-98d5-6caa16ff5aed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device:  cpu \n","Epoch 1/5 loss: 1.734\n","Epoch 2/5 loss: 1.581\n","Epoch 3/5 loss: 1.560\n","Epoch 4/5 loss: 1.552\n","Epoch 5/5 loss: 1.542\n"]}]},{"cell_type":"markdown","source":["# Testing"],"metadata":{"id":"0tRRL4gDQlFN"}},{"cell_type":"code","source":["correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","  for data in test_loader:\n","    images, labels = data\n","    images, labels = images.to(device), labels.to(device)\n","\n","    outputs = transformer(images)\n","\n","    _, predicted = torch.max(outputs.data, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","  print(f'\\nModel Accuracy: {100 * correct // total} %')"],"metadata":{"id":"ZwXO-XiEQnFy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710429010372,"user_tz":360,"elapsed":3122,"user":{"displayName":"Matt Nguyen","userId":"06637147104061724739"}},"outputId":"31e591e9-da67-4807-d3f7-b82ecb265076"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Model Accuracy: 92 %\n"]}]}]}